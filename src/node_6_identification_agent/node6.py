from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from prompt.prompt import Prompt_Identification_AT
from tool.Call_LLM import call_LLM
import json
import re
from datetime import datetime
import os

base_llm = call_LLM()
CHROMA_DIR = "./chroma_mitre_mpnet"
EMBED_MODEL = "sentence-transformers/all-mpnet-base-v2"
RERANK_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
OUTPUT_DIR = "./output"  # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£

# Load embedding model v√† vector database
print("üîπ [Node6] ƒêang t·∫£i embedding model v√† Chroma DB...")
embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding_model)

# Load reranker model
print("üîπ [Node6] ƒêang t·∫£i reranker model...")
reranker = CrossEncoder(RERANK_MODEL)

def clean_html(text):
    """X√≥a tag HTML cho m√¥ t·∫£ MITRE"""
    return re.sub(r"<.*?>", "", text or "")

def get_candidate_techniques(query, top_k=5):
    """
    Truy v·∫•n vector database ƒë·ªÉ l·∫•y top_k k·ªπ thu·∫≠t AT,
    sau ƒë√≥ rerank l·∫°i v√† tr·∫£ v·ªÅ danh s√°ch c√°c k·ªπ thu·∫≠t ph√π h·ª£p nh·∫•t.
    
    Returns:
        str: Chu·ªói JSON ch·ª©a th√¥ng tin c√°c k·ªπ thu·∫≠t AT candidate
    """
    # Retrieval: l·∫•y top_k k·∫øt qu·∫£ t·ª´ embedding similarity
    results = vectordb.similarity_search_with_score(query, k=top_k)
    
    if not results:
        return "[]"
    
    candidates = [(doc.page_content, doc.metadata, score) for doc, score in results]
    
    # T√≠nh ƒëi·ªÉm rerank (cross-encoder) - ch·∫°y t·ª´ng c·∫∑p m·ªôt
    pairs = [[query, c[0]] for c in candidates]
    rerank_scores = [float(reranker.predict([pair])[0]) for pair in pairs]
    
    # T·∫°o danh s√°ch k·ªπ thu·∫≠t v·ªõi ƒëi·ªÉm s·ªë
    techniques_info = []
    for i, (text, meta, emb_score) in enumerate(candidates):
        technique_id = meta.get('technique_id', 'Unknown')
        description = clean_html(text).replace("\n", " ").strip()
        
        techniques_info.append({
            "technique_id": technique_id,
            "description": description,
            "rerank_score": rerank_scores[i]
        })
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm rerank gi·∫£m d·∫ßn
    techniques_info.sort(key=lambda x: x["rerank_score"], reverse=True)
    
    return json.dumps(techniques_info, ensure_ascii=False, indent=2)

def Node6(state: dict):
    mapped_sentences = state['mapped_sentences']
    all_AT = []
    results_simple = []  # L∆∞u k·∫øt qu·∫£ ƒë∆°n gi·∫£n
    
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    for idx, sentence in enumerate(mapped_sentences):
        # L·∫•y top 10 k·ªπ thu·∫≠t AT t·ª´ vector database v√† rerank
        candidate_techniques = get_candidate_techniques(sentence, top_k=10)
        
        # T·∫°o prompt v·ªõi th√¥ng tin c√°c k·ªπ thu·∫≠t candidate
        messages_system = SystemMessage(content=Prompt_Identification_AT)
        
        # Th√™m th√¥ng tin c√°c k·ªπ thu·∫≠t candidate v√†o user message
        user_content = f"""Sentence to analyze:
{sentence}

Candidate MITRE ATT&CK Techniques (retrieved and reranked from vector database):
{candidate_techniques}

Based on the sentence and the candidate techniques above, identify which technique(s) best match the described behavior."""
        
        messages_user = HumanMessage(content=user_content)

        messages = [
            messages_system,
            messages_user
        ]
        
        identified_techniques = []
        try:
            response = base_llm.invoke(messages)
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # Parse response - k·ª≥ v·ªçng l√† list ho·∫∑c "None"
            if response_content.strip().lower() != "none":
                # Th·ª≠ parse JSON n·∫øu l√† list
                try:
                    parsed = json.loads(response_content)
                    if isinstance(parsed, list) and len(parsed) > 0:
                        identified_techniques = parsed
                        all_AT.append(parsed)
                except:
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, th·ª≠ extract t·ª´ text
                    # T√¨m c√°c pattern T#### trong response
                    techniques = re.findall(r'T\d{4}', response_content)
                    if techniques:
                        identified_techniques = techniques
                        all_AT.append(techniques)
            
            # L∆∞u k·∫øt qu·∫£ ƒë∆°n gi·∫£n: ch·ªâ id, sentence v√† AT
            results_simple.append({
                "id": idx + 1,
                "sentence": sentence,
                "AT": identified_techniques
            })
            
        except Exception as e:
            print(f"Error processing sentence {idx+1}: {e}")
            results_simple.append({
                "id": idx + 1,
                "sentence": sentence,
                "AT": [],
                "link": state['input']
            })
    
    # T√¨m s·ªë th·ª© t·ª± file ti·∫øp theo
    existing_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('_AT_result.json')]
    if existing_files:
        # L·∫•y s·ªë l·ªõn nh·∫•t t·ª´ t√™n file
        numbers = []
        for f in existing_files:
            match = re.match(r'(\d+)_AT_result\.json', f)
            if match:
                numbers.append(int(match.group(1)))
        next_number = max(numbers) + 1 if numbers else 1
    else:
        next_number = 1
    
    # Ghi k·∫øt qu·∫£ ra file v·ªõi s·ªë th·ª© t·ª±
    output_file = os.path.join(OUTPUT_DIR, f"{next_number:02d}_AT_result.json")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_simple, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìÑ [Node6] K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_file}")
            
    return {"AT": all_AT}
    